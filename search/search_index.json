{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to MkDocs\n\n\nFor full documentation visit \nmkdocs.org\n.\n\n\nCommands\n\n\n\n\nmkdocs new [dir-name]\n - Create a new project.\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nProject layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-mkdocs", 
            "text": "For full documentation visit  mkdocs.org .", 
            "title": "Welcome to MkDocs"
        }, 
        {
            "location": "/#commands", 
            "text": "mkdocs new [dir-name]  - Create a new project.  mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.", 
            "title": "Commands"
        }, 
        {
            "location": "/#project-layout", 
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Project layout"
        }, 
        {
            "location": "/about/", 
            "text": "Dual Ascent\n\n\nProblem to solve:\n$$\n\\mbox{minimize}  \\ \\ f(x) \\ \\mbox{subject  to} \\ \\ Ax=b  \\x\\ \\in\\ \\mathbb{R}^{n} \\ where \\ A\\  \\in\\  \\mathbb{R}^{m\\times n}\\ and\\ f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\ is\\ convex\n$$\n\n\nThe Lagrangian is:\n$$\nL(x,y)=f(x)+y^{T}(Ax-b)\n$$\nThe dual function is:\n$$\ng(y) =\\inf\\limits_{x} L(x,y)=-f^{\\ast}(-A^{T}y)-b^{T}y\n$$\nwhere $y$ is the dual variable or Lagrange multiplier, and the $f^*$ is the conjugate of the $f$. This function says taht $g(y)$ is convex of $y$.\n\n\nThe definition of $f^{\n}(y) $:\n$$\nf^{\n}(y):\\sup\\limits_{x\\in domf}:(y^{T}x-f(x))\n$$\n$Proof$:\n$$\n\\begin{align}\ng(y) \n= \\inf\\limits_{x} L(x,y)=\\inf\\limits_{x}f(x) +y^{T}Ax-y^{T}b\\\n=-\\sup\\limits_{x}-f(x)+y^{T}(-Ax)+y^{T}b\\\n=-f^{*}(-A^{T}y)-y^{T}b\n\\end{align}\n$$\nNow, the dual problem is \n$$\n\\mbox{maximize} \\ g(y)\n$$\nwith variable $y\\in \\mathbb{R}^{m}$.\n\n\nAssuming the strong duality holds. the optimal values of the primal and dual problems are the same.\n\n\nThe primal optimal point $x^{\n}$ form a dual optimal point $y^{\n}$ as \n$$\nx^{\n}=arg\\min\\limits_{x} L(x,y^{\n}),\n$$\nprovided that there is only one minimizer of $L(x,y^{*})$. \n\n\nThe notation $\\mbox{argmin}_{x}F(x)$ denotes any minimizer of $F$. \n\n\nIn the $dual\\ ascent\\ method$, we use \ngreadient ascent\n to solve dual problem. Assuming $g$ is differentiable. We first find $x^{+}=argmin_{x}L(x,y)$, then $\\nabla g(y)=Ax^{+}-b$, which is the residual for the equality constraint. \n$$\nx^{k+1}:=arg\\min\\limits_{x}L(x,y^{k})\\\ny^{k+1}:=y^{k}+\\alpha^{k}(Ax^{k+1}-b),\n$$\nwhere $\\alpha^{k}\n0$ is a step size, the $k$ is the iteration counter. With appropriate choice of $\\alpha^{k}$, $g(y^{k+1})\ng(y^{k})$.", 
            "title": "About"
        }, 
        {
            "location": "/about/#dual-ascent", 
            "text": "Problem to solve:\n$$\n\\mbox{minimize}  \\ \\ f(x) \\ \\mbox{subject  to} \\ \\ Ax=b  \\x\\ \\in\\ \\mathbb{R}^{n} \\ where \\ A\\  \\in\\  \\mathbb{R}^{m\\times n}\\ and\\ f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\ is\\ convex\n$$  The Lagrangian is:\n$$\nL(x,y)=f(x)+y^{T}(Ax-b)\n$$\nThe dual function is:\n$$\ng(y) =\\inf\\limits_{x} L(x,y)=-f^{\\ast}(-A^{T}y)-b^{T}y\n$$\nwhere $y$ is the dual variable or Lagrange multiplier, and the $f^*$ is the conjugate of the $f$. This function says taht $g(y)$ is convex of $y$.  The definition of $f^{ }(y) $:\n$$\nf^{ }(y):\\sup\\limits_{x\\in domf}:(y^{T}x-f(x))\n$$\n$Proof$:\n$$\n\\begin{align}\ng(y)  = \\inf\\limits_{x} L(x,y)=\\inf\\limits_{x}f(x) +y^{T}Ax-y^{T}b\\ =-\\sup\\limits_{x}-f(x)+y^{T}(-Ax)+y^{T}b\\ =-f^{*}(-A^{T}y)-y^{T}b\n\\end{align}\n$$\nNow, the dual problem is \n$$\n\\mbox{maximize} \\ g(y)\n$$\nwith variable $y\\in \\mathbb{R}^{m}$.  Assuming the strong duality holds. the optimal values of the primal and dual problems are the same.  The primal optimal point $x^{ }$ form a dual optimal point $y^{ }$ as \n$$\nx^{ }=arg\\min\\limits_{x} L(x,y^{ }),\n$$\nprovided that there is only one minimizer of $L(x,y^{*})$.   The notation $\\mbox{argmin}_{x}F(x)$ denotes any minimizer of $F$.   In the $dual\\ ascent\\ method$, we use  greadient ascent  to solve dual problem. Assuming $g$ is differentiable. We first find $x^{+}=argmin_{x}L(x,y)$, then $\\nabla g(y)=Ax^{+}-b$, which is the residual for the equality constraint. \n$$\nx^{k+1}:=arg\\min\\limits_{x}L(x,y^{k})\\\ny^{k+1}:=y^{k}+\\alpha^{k}(Ax^{k+1}-b),\n$$\nwhere $\\alpha^{k} 0$ is a step size, the $k$ is the iteration counter. With appropriate choice of $\\alpha^{k}$, $g(y^{k+1}) g(y^{k})$.", 
            "title": "Dual Ascent"
        }
    ]
}