{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to MkDocs\n\n\nFor full documentation visit \nmkdocs.org\n.\n\n\nCommands\n\n\n\n\nmkdocs new [dir-name]\n - Create a new project.\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nProject layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-mkdocs", 
            "text": "For full documentation visit  mkdocs.org .", 
            "title": "Welcome to MkDocs"
        }, 
        {
            "location": "/#commands", 
            "text": "mkdocs new [dir-name]  - Create a new project.  mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.", 
            "title": "Commands"
        }, 
        {
            "location": "/#project-layout", 
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Project layout"
        }, 
        {
            "location": "/dual_ascent/", 
            "text": "Dual Ascent\n\n\nProblem to solve:\n\n\n\\mbox{minimize}  \\ \\ f(x) \\\\ \\mbox{subject  to} \\ \\ Ax=b  \\\\x\\ \\in\\ \\mathbb{R}^{n} \\ where \\ A\\  \\in\\  \\mathbb{R}^{m\\times n}\\ and\\ f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\ is\\ convex\n\n\n\n\n\nThe Lagrangian is:\n\n\nL(x,y)=f(x)+y^{T}(Ax-b)\n\n\nThe dual function is:\n\n\ng(y) =\\inf\\limits_{x} L(x,y)=-f^{\\ast}(-A^{T}y)-b^{T}y\n\n\nwhere \ny\n is the dual variable or Lagrange multiplier, and the \nf^*\n is the conjugate of the \nf\n. This function says taht \ng(y)\n is convex of \ny\n.\n\n\nThe definition of \nf^{*}(y) \n:\n\n\nf^{*}(y):\\sup\\limits_{x\\in domf}:(y^{T}x-f(x))\n\n\n\n\\color{blue}{Proof}\n:\n\n\n\\begin{align}\ng(y) &= \\inf\\limits_{x} L(x,y)=\\inf\\limits_{x}f(x) +y^{T}Ax-y^{T}b\\\\&=-\\sup\\limits_{x}-f(x)+y^{T}(-Ax)+y^{T}b\\\\&=-f^{*}(-A^{T}y)-y^{T}b\n\\end{align}\n\n\nNow, the dual problem is \n\n\n\\mbox{maximize} \\ g(y)\n\n\nwith variable \ny\\in \\mathbb{R}^{m}\n.\n\n\nAssuming the strong duality holds. the optimal values of the primal and dual problems are the same.\n\n\nThe primal optimal point \nx^{*}\n form a dual optimal point \ny^{*}\n as \n\n\nx^{*}=arg\\min\\limits_{x} L(x,y^{*}),\n\n\nprovided that there is only one minimizer of \nL(x,y^{*})\n. \n\n\nThe notation \n\\mbox{argmin}_{x}F(x)\n denotes any minimizer of \nF\n. \n\n\nIn the \ndual\\ ascent\\ method\n, we use \ngreadient ascent\n to solve dual problem. Assuming \ng\n is differentiable. We first find \nx^{+}=argmin_{x}L(x,y)\n, then the gradient \n\\nabla g(y)=Ax^{+}-b\n, which is the residual for the equality constraint. \n\n\nx^{k+1}:=arg\\min\\limits_{x}L(x,y^{k})\\\\\ny^{k+1}:=y^{k}+\\alpha^{k}(Ax^{k+1}-b),\n\n\nwhere \n\\alpha^{k}>0\n is a step size, the \nk\n is the iteration counter. With appropriate choice of \n\\alpha^{k}\n, \ng(y^{k+1})>g(y^{k})\n. \n\n\nThe principle is \n\\max\\limits_{y}g(y)=\\max\\limits_{x}L(x,y) \n, optimizing x and y alternatively to reach \nL(x^{*},y^{*})\n. \n\n\nWhen \ng\n is not differentiable\n\n\nThe residual \nAx^{k+1}-b\n is not the gradient of \ng\n, but the negative of a subgradient of \n-g\n.  It is often that \ng(y^{k+1})\\ngtr g(y^{k})\n. The algorithm is usually called the \ndual\\ subgradient\\ method\n. \n\n\nIf \n\\alpha^{k}\n is chosen appropriately and several other assumptions hold, then \nx^{k}\n converges to an optimal point and \ny^{k}\n converges to an opotimal dual point. \n\n\nHowever, these assumptions \ndo not hold\n in many cases. For example,  if \nf\n is a nonzero affine function of any component of x, \nL\n is unbounded below in \nx\n for most \ny\n. \n\n\nDual Decomposition\n\n\nThe \n\\color{blue}{\\mbox{major benefit}}\n of the dual ascent is that it can lead to a decentralized algorithm in some cases.\n\n\nSuppose, \nf\n is \nseparable\n, meaning that\n\n\nf(x)=\\sum_{i=1}^{N}f_{i}(x_i),\n\n\nwhere \nx=(x_1,\u2026,x_N)\n and the variables \nx_i\\in\\mathbb{R}^{n_i}\n are subvectors of \nx\n. \n\n\nPartitioning the matrix \nA\n conformably as \n\n\nA=[A_1\\cdot\\cdot\\cdot A_N],\n\n\nso \nAx=\\sum_{i=1}^{N}A_ix_i\n, the Lagrangian is \n\n\nL(x,y )=\\sum_{i=1}^{N}L_{i}(x_i,y)=\\sum_{i=1}^{N}(f_{i}+y^TA_ix_i-(1/N)y^Tb)\\\\\\mbox{such that } L(x,y)=f(x)+y^{T}(Ax-b)\n\n\nwhich is also separable in \nx\n. \n\\color{blue}{\\mbox{The x-minimization step splits into N separate prolems that can be solved in parallel.}}\n\n\n\n\nNow, the algorithm is \n\n\nx^{k+1}_i:=arg\\min\\limits_{x_i}L_i(x_i,y^k)\\\\y^{k+1}:=y^k+\\alpha^{k}(Ax^{k+1}-b).\n\n\nIn this case, we refer to the dual ascent mehtod as \ndual decomposition\n. Dual decomposition is used to do dual acent method for separable \nf(x)\n. Dual decomposition is distributed dual ascent method. \n\n\nImplementation:\n\n\nEach iteration of the dual decomposition methods inclued \nbroadcast\n and \ngather\n operation.  First, \nA_ix_i^{k+1}\n are collected(gathered). Second, compute the residual \nAx^{k+1}-b\n.  Then, compute the (global) dual variable \ny^{k+1}\n. Finally, distributed (broadcast)  to the processors that carry out the \nN\n individual \nx_i\n minimization steps.", 
            "title": "Daul Ascent"
        }, 
        {
            "location": "/dual_ascent/#dual-ascent", 
            "text": "Problem to solve: \n\\mbox{minimize}  \\ \\ f(x) \\\\ \\mbox{subject  to} \\ \\ Ax=b  \\\\x\\ \\in\\ \\mathbb{R}^{n} \\ where \\ A\\  \\in\\  \\mathbb{R}^{m\\times n}\\ and\\ f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\ is\\ convex   The Lagrangian is: \nL(x,y)=f(x)+y^{T}(Ax-b) \nThe dual function is: \ng(y) =\\inf\\limits_{x} L(x,y)=-f^{\\ast}(-A^{T}y)-b^{T}y \nwhere  y  is the dual variable or Lagrange multiplier, and the  f^*  is the conjugate of the  f . This function says taht  g(y)  is convex of  y .  The definition of  f^{*}(y)  : \nf^{*}(y):\\sup\\limits_{x\\in domf}:(y^{T}x-f(x))  \\color{blue}{Proof} : \n\\begin{align}\ng(y) &= \\inf\\limits_{x} L(x,y)=\\inf\\limits_{x}f(x) +y^{T}Ax-y^{T}b\\\\&=-\\sup\\limits_{x}-f(x)+y^{T}(-Ax)+y^{T}b\\\\&=-f^{*}(-A^{T}y)-y^{T}b\n\\end{align} \nNow, the dual problem is  \n\\mbox{maximize} \\ g(y) \nwith variable  y\\in \\mathbb{R}^{m} .  Assuming the strong duality holds. the optimal values of the primal and dual problems are the same.  The primal optimal point  x^{*}  form a dual optimal point  y^{*}  as  \nx^{*}=arg\\min\\limits_{x} L(x,y^{*}), \nprovided that there is only one minimizer of  L(x,y^{*}) .   The notation  \\mbox{argmin}_{x}F(x)  denotes any minimizer of  F .   In the  dual\\ ascent\\ method , we use  greadient ascent  to solve dual problem. Assuming  g  is differentiable. We first find  x^{+}=argmin_{x}L(x,y) , then the gradient  \\nabla g(y)=Ax^{+}-b , which is the residual for the equality constraint.  \nx^{k+1}:=arg\\min\\limits_{x}L(x,y^{k})\\\\\ny^{k+1}:=y^{k}+\\alpha^{k}(Ax^{k+1}-b), \nwhere  \\alpha^{k}>0  is a step size, the  k  is the iteration counter. With appropriate choice of  \\alpha^{k} ,  g(y^{k+1})>g(y^{k}) .   The principle is  \\max\\limits_{y}g(y)=\\max\\limits_{x}L(x,y)  , optimizing x and y alternatively to reach  L(x^{*},y^{*}) .", 
            "title": "Dual Ascent"
        }, 
        {
            "location": "/dual_ascent/#when-g-is-not-differentiable", 
            "text": "The residual  Ax^{k+1}-b  is not the gradient of  g , but the negative of a subgradient of  -g .  It is often that  g(y^{k+1})\\ngtr g(y^{k}) . The algorithm is usually called the  dual\\ subgradient\\ method .   If  \\alpha^{k}  is chosen appropriately and several other assumptions hold, then  x^{k}  converges to an optimal point and  y^{k}  converges to an opotimal dual point.   However, these assumptions  do not hold  in many cases. For example,  if  f  is a nonzero affine function of any component of x,  L  is unbounded below in  x  for most  y .", 
            "title": "When g is not differentiable"
        }, 
        {
            "location": "/dual_ascent/#dual-decomposition", 
            "text": "The  \\color{blue}{\\mbox{major benefit}}  of the dual ascent is that it can lead to a decentralized algorithm in some cases.  Suppose,  f  is  separable , meaning that \nf(x)=\\sum_{i=1}^{N}f_{i}(x_i), \nwhere  x=(x_1,\u2026,x_N)  and the variables  x_i\\in\\mathbb{R}^{n_i}  are subvectors of  x .   Partitioning the matrix  A  conformably as  \nA=[A_1\\cdot\\cdot\\cdot A_N], \nso  Ax=\\sum_{i=1}^{N}A_ix_i , the Lagrangian is  \nL(x,y )=\\sum_{i=1}^{N}L_{i}(x_i,y)=\\sum_{i=1}^{N}(f_{i}+y^TA_ix_i-(1/N)y^Tb)\\\\\\mbox{such that } L(x,y)=f(x)+y^{T}(Ax-b) \nwhich is also separable in  x .  \\color{blue}{\\mbox{The x-minimization step splits into N separate prolems that can be solved in parallel.}}   Now, the algorithm is  \nx^{k+1}_i:=arg\\min\\limits_{x_i}L_i(x_i,y^k)\\\\y^{k+1}:=y^k+\\alpha^{k}(Ax^{k+1}-b). \nIn this case, we refer to the dual ascent mehtod as  dual decomposition . Dual decomposition is used to do dual acent method for separable  f(x) . Dual decomposition is distributed dual ascent method.", 
            "title": "Dual Decomposition"
        }, 
        {
            "location": "/dual_ascent/#implementation", 
            "text": "Each iteration of the dual decomposition methods inclued  broadcast  and  gather  operation.  First,  A_ix_i^{k+1}  are collected(gathered). Second, compute the residual  Ax^{k+1}-b .  Then, compute the (global) dual variable  y^{k+1} . Finally, distributed (broadcast)  to the processors that carry out the  N  individual  x_i  minimization steps.", 
            "title": "Implementation:"
        }
    ]
}