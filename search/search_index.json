{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to MkDocs\n\n\nFor full documentation visit \nmkdocs.org\n.\n\n\nCommands\n\n\n\n\nmkdocs new [dir-name]\n - Create a new project.\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nProject layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-mkdocs", 
            "text": "For full documentation visit  mkdocs.org .", 
            "title": "Welcome to MkDocs"
        }, 
        {
            "location": "/#commands", 
            "text": "mkdocs new [dir-name]  - Create a new project.  mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.", 
            "title": "Commands"
        }, 
        {
            "location": "/#project-layout", 
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Project layout"
        }, 
        {
            "location": "/about/", 
            "text": "Dual Ascent\n\n\nProblem to solve:\n\n\n\\mbox{minimize}  \\ \\ f(x) \\\\ \\mbox{subject  to} \\ \\ Ax=b  \\\\x\\ \\in\\ \\mathbb{R}^{n} \\ where \\ A\\  \\in\\  \\mathbb{R}^{m\\times n}\\ and\\ f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\ is\\ convex\n\n\n\n\n\nThe Lagrangian is:\n\n\nL(x,y)=f(x)+y^{T}(Ax-b)\n\n\nThe dual function is:\n\n\ng(y) =\\inf\\limits_{x} L(x,y)=-f^{\\ast}(-A^{T}y)-b^{T}y\n\n\nwhere \ny\n is the dual variable or Lagrange multiplier, and the \nf^*\n is the conjugate of the \nf\n. This function says taht \ng(y)\n is convex of \ny\n.\n\n\nThe definition of \nf^{*}(y) \n:\n\n\nf^{*}(y):\\sup\\limits_{x\\in domf}:(y^{T}x-f(x))\n\n\n\nProof\n:\n\n\n\\begin{align}\ng(y) &= \\inf\\limits_{x} L(x,y)=\\inf\\limits_{x}f(x) +y^{T}Ax-y^{T}b\\\\&=-\\sup\\limits_{x}-f(x)+y^{T}(-Ax)+y^{T}b\\\\&=-f^{*}(-A^{T}y)-y^{T}b\n\\end{align}\n\n\nNow, the dual problem is \n\n\n\\mbox{maximize} \\ g(y)\n\n\nwith variable \ny\\in \\mathbb{R}^{m}\n.\n\n\nAssuming the strong duality holds. the optimal values of the primal and dual problems are the same.\n\n\nThe primal optimal point \nx^{*}\n form a dual optimal point \ny^{*}\n as \n\n\nx^{*}=arg\\min\\limits_{x} L(x,y^{*}),\n\n\nprovided that there is only one minimizer of \nL(x,y^{*})\n. \n\n\nThe notation \n\\mbox{argmin}_{x}F(x)\n denotes any minimizer of \nF\n. \n\n\nIn the \ndual\\ ascent\\ method\n, we use \ngreadient ascent\n to solve dual problem. Assuming \ng\n is differentiable. We first find \nx^{+}=argmin_{x}L(x,y)\n, then \n\\nabla g(y)=Ax^{+}-b\n, which is the residual for the equality constraint. \n\n\nx^{k+1}:=arg\\min\\limits_{x}L(x,y^{k})\\\\\ny^{k+1}:=y^{k}+\\alpha^{k}(Ax^{k+1}-b),\n\n\nwhere \n\\alpha^{k}>0\n is a step size, the \nk\n is the iteration counter. With appropriate choice of \n\\alpha^{k}\n, \ng(y^{k+1})>g(y^{k})\n.", 
            "title": "About"
        }, 
        {
            "location": "/about/#dual-ascent", 
            "text": "Problem to solve: \n\\mbox{minimize}  \\ \\ f(x) \\\\ \\mbox{subject  to} \\ \\ Ax=b  \\\\x\\ \\in\\ \\mathbb{R}^{n} \\ where \\ A\\  \\in\\  \\mathbb{R}^{m\\times n}\\ and\\ f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}\\ is\\ convex   The Lagrangian is: \nL(x,y)=f(x)+y^{T}(Ax-b) \nThe dual function is: \ng(y) =\\inf\\limits_{x} L(x,y)=-f^{\\ast}(-A^{T}y)-b^{T}y \nwhere  y  is the dual variable or Lagrange multiplier, and the  f^*  is the conjugate of the  f . This function says taht  g(y)  is convex of  y .  The definition of  f^{*}(y)  : \nf^{*}(y):\\sup\\limits_{x\\in domf}:(y^{T}x-f(x))  Proof : \n\\begin{align}\ng(y) &= \\inf\\limits_{x} L(x,y)=\\inf\\limits_{x}f(x) +y^{T}Ax-y^{T}b\\\\&=-\\sup\\limits_{x}-f(x)+y^{T}(-Ax)+y^{T}b\\\\&=-f^{*}(-A^{T}y)-y^{T}b\n\\end{align} \nNow, the dual problem is  \n\\mbox{maximize} \\ g(y) \nwith variable  y\\in \\mathbb{R}^{m} .  Assuming the strong duality holds. the optimal values of the primal and dual problems are the same.  The primal optimal point  x^{*}  form a dual optimal point  y^{*}  as  \nx^{*}=arg\\min\\limits_{x} L(x,y^{*}), \nprovided that there is only one minimizer of  L(x,y^{*}) .   The notation  \\mbox{argmin}_{x}F(x)  denotes any minimizer of  F .   In the  dual\\ ascent\\ method , we use  greadient ascent  to solve dual problem. Assuming  g  is differentiable. We first find  x^{+}=argmin_{x}L(x,y) , then  \\nabla g(y)=Ax^{+}-b , which is the residual for the equality constraint.  \nx^{k+1}:=arg\\min\\limits_{x}L(x,y^{k})\\\\\ny^{k+1}:=y^{k}+\\alpha^{k}(Ax^{k+1}-b), \nwhere  \\alpha^{k}>0  is a step size, the  k  is the iteration counter. With appropriate choice of  \\alpha^{k} ,  g(y^{k+1})>g(y^{k}) .", 
            "title": "Dual Ascent"
        }
    ]
}